services:
  spark-master:
    build:
      context: .
      dockerfile: dockerfile.spark
    container_name: "spark-master"
    hostname: 'spark-master-host'
    environment:
      - SPARK_MODE=master
      - SPARK_LOCAL_IP=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "${SPARK_MASTER_PORT}:7077"
      - "${SPARK_MASTER_UI_PORT}:8080"
    extra_hosts:
      - "host.docker.internal:${SERVER_IP}"
    volumes:
      - ./src:/src
      - ./data:/data
      - ./output:/output

  spark-worker-a:
    build:
      context: .
      dockerfile: dockerfile.spark
    ports:
      - "${SPARK_WORKER_A_UI_PORT}:8080"
      - "${SPARK_WORKER_A_PORT}:7000"
    extra_hosts:
      - "host.docker.internal:${SERVER_IP}"
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_USER=spark


  spark-worker-b:
    build:
      context: .
      dockerfile: dockerfile.spark
    ports:
      - "${SPARK_WORKER_B_UI_PORT}:8080"
      - "${SPARK_WORKER_B_PORT}:7000"
    extra_hosts:
      - "host.docker.internal:${SERVER_IP}"
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

  # spark-submit:
  #   build:
  #     context: .
  #     dockerfile: dockerfile.spark_submit
  #   image: spark-submit
  #   container_name: "spark-submit"
  #   # entrypoint: ["bash", "-c", "/opt/spark/run.sh"]
  #   environment:
  #     - SPARK_MODE=master
  #     # - SPARK_LOCAL_IP=${SERVER_IP}
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   # ports:
  #   #   - "${SPARK_SUBMIT_PORT}:7077"
  #   #   - "${SPARK_SUBMIT_UI_PORT}:8080"
  #   extra_hosts:
  #     - "host.docker.internal:${SERVER_IP}"
  #   # network_mode: host
  #   volumes:
  #     - ./app/:/opt/spark-job/
  #     # - ./src:/src
  #     # - ./data:/data
  #     # - ./output:/output