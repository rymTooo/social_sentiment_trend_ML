{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "import mlflow\n",
    "import os\n",
    "from mlflow.models import infer_signature\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "# from tokenizer import Thai_tokenizer\n",
    "import requests\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Typhoon_model:\n",
    "    def __init__(self, model_name=\"scb10x/llama-3-typhoon-v1.5-8b-instruct\",num_labels=3):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.pipeline = pipeline(task = \"text-generation\", model=self.model, tokenizer=self.tokenizer, device=0)\n",
    "        \n",
    "\n",
    "\n",
    "    # def make_request(self, message, url, api_key):\n",
    "    #     client = ChatOpenAI(base_url='https://api.opentyphoon.ai/v1',\n",
    "    #                         model='typhoon-instruct',\n",
    "    #                         api_key=api_key)\n",
    "    #     resp = client.invoke([HumanMessage(content=message)])\n",
    "    #     print(resp.content)\n",
    "    \n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    def predict(self, input_text):\n",
    "        prediction = self.pipeline(input_text) #[0]\n",
    "        # print(prediction)\n",
    "        # if prediction['score'] <= 0.8:\n",
    "        #     prediction['label'] = 'NEUTRAL'\n",
    "\n",
    "        # prediction = self.make_request(input_text, self.url, self.api_key)\n",
    "        return prediction\n",
    "\n",
    "    def train(self, train_dataset, test_dataset, output_dir=\"./results\", epochs=3, batch_size=16, learning_rate=1e-5):\n",
    "        # Tokenize datasets\n",
    "        tokenized_train = train_dataset.map(self.tokenize_function, batched=True)\n",
    "        tokenized_test = test_dataset.map(self.tokenize_function, batched=True)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_test,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "    def evaluate(self, eval_dataset):\n",
    "        # Tokenize dataset\n",
    "        tokenized_eval = eval_dataset.map(self.tokenize_function, batched=True)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            per_device_eval_batch_size=16,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            eval_dataset=tokenized_eval,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "        metrics = trainer.evaluate()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "typhoon = Typhoon_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral. confident score : 0.75.\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \n",
    "                \"\"\"\n",
    "                 You are a good assistant named Typhoon.\n",
    "                 Your answer can only be 'good' or 'bad' or 'neutral'.\n",
    "                 You will choose between three answer based on the sentiment of the input from user.\n",
    "                 You will also give the confidence score of your answer in the format of 'confident score : [score]'.\n",
    "                 The confident score will be between 0 and 1. \n",
    "                 1 being the most confident and 0 being the least confident.\n",
    "                \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"#แพทย์จุฬา #คณะแพทย์ #แพทยศาสตร์ #แพทย์อินเตอร์ #จุฬา #cumedi #medical #medicine #chula #medchula #chulalongkorn. Image. 9:51 AM · Jul 8, 2020.\",\n",
    "        }\n",
    "    ]\n",
    "print(typhoon.pipeline(messages)[0]['generated_text'][2]['content'])\n",
    "\n",
    "# You are a good assistant named Typhoon.\n",
    "#                 Your answer can only be in the format of {'good':[score],'bad':[score],'neutral':[score]}.\n",
    "#                 You will assign the score of each topic based on the sentiment of the input.\n",
    "#                 The score will be between 0 and 1. 1 being the most confident and 0 being least confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
